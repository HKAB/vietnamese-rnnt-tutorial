{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fft import rfft as fft\n",
    "from scipy.signal import check_COLA, get_window\n",
    "import torch.nn as nn\n",
    "\n",
    "support_clp_op = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from models.encoder import AudioEncoder\n",
    "from models.decoder import Decoder\n",
    "from models.jointer import Jointer\n",
    "\n",
    "from constants import SAMPLE_RATE, N_FFT, HOP_LENGTH, N_MELS\n",
    "from constants import RNNT_BLANK, PAD, VOCAB_SIZE, TOKENIZER_MODEL_PATH, MAX_SYMBOLS\n",
    "from constants import ATTENTION_CONTEXT_SIZE\n",
    "from constants import N_STATE, N_LAYER, N_HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/wp/checkpoints/rnnt-latest.ckpt\", map_location=\"cpu\", weights_only=True )\n",
    "\n",
    "encoder_weight = {}\n",
    "decoder_weight = {}\n",
    "joint_weight = {}\n",
    "\n",
    "for k, v in checkpoint['state_dict'].items():\n",
    "    if 'alibi' in k:\n",
    "        continue\n",
    "    if 'encoder' in k:\n",
    "        encoder_weight[k.replace('encoder.', '')] = v\n",
    "    elif 'decoder' in k:\n",
    "        decoder_weight[k.replace('decoder.', '')] = v\n",
    "    elif 'joint' in k:\n",
    "        joint_weight[k.replace('joint.', '')] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = AudioEncoder(\n",
    "    n_mels=N_MELS,\n",
    "    n_state=N_STATE,\n",
    "    n_head=N_HEAD,\n",
    "    n_layer=N_LAYER,\n",
    "    att_context_size=ATTENTION_CONTEXT_SIZE\n",
    ")\n",
    "\n",
    "decoder = Decoder(vocab_size=VOCAB_SIZE + 1)\n",
    "\n",
    "joint = Jointer(vocab_size=VOCAB_SIZE + 1)\n",
    "\n",
    "encoder.load_state_dict(encoder_weight, strict=False)\n",
    "decoder.load_state_dict(decoder_weight, strict=False)\n",
    "joint.load_state_dict(joint_weight, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jointer(\n",
       "  (fc): Linear(in_features=768, out_features=1025, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "joint = joint.to(DEVICE)\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "joint.eval()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/echocatzh/conv-stft/blob/master/conv_stft/conv_stft.py\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportable STFT from https://github.com/echocatzh/conv-stft/blob/master/conv_stft/conv_stft.py\n",
>>>>>>> b00ae34 (update code)
    "class STFT(torch.nn.Module):\n",
    "    def __init__(self, win_len=1024, win_hop=512, fft_len=1024,\n",
    "                 enframe_mode='continue', win_type='hann',\n",
    "                 win_sqrt=False, pad_center=True):\n",
    "        \"\"\"\n",
    "        Implement of STFT using 1D convolution and 1D transpose convolutions.\n",
    "        Implement of framing the signal in 2 ways, `break` and `continue`.\n",
    "        `break` method is a kaldi-like framing.\n",
    "        `continue` method is a librosa-like framing.\n",
    "\n",
    "        More information about `perfect reconstruction`:\n",
    "        1. https://ww2.mathworks.cn/help/signal/ref/stft.html\n",
    "        2. https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.get_window.html\n",
    "\n",
    "        Args:\n",
    "            win_len (int): Number of points in one frame.  Defaults to 1024.\n",
    "            win_hop (int): Number of framing stride. Defaults to 512.\n",
    "            fft_len (int): Number of DFT points. Defaults to 1024.\n",
    "            enframe_mode (str, optional): `break` and `continue`. Defaults to 'continue'.\n",
    "            win_type (str, optional): The type of window to create. Defaults to 'hann'.\n",
    "            win_sqrt (bool, optional): using square root window. Defaults to True.\n",
    "            pad_center (bool, optional): `perfect reconstruction` opts. Defaults to True.\n",
    "        \"\"\"\n",
    "        super(STFT, self).__init__()\n",
    "        assert enframe_mode in ['break', 'continue']\n",
    "        assert fft_len >= win_len\n",
    "        self.win_len = win_len\n",
    "        self.win_hop = win_hop\n",
    "        self.fft_len = fft_len\n",
    "        self.mode = enframe_mode\n",
    "        self.win_type = win_type\n",
    "        self.win_sqrt = win_sqrt\n",
    "        self.pad_center = pad_center\n",
    "        self.pad_amount = self.fft_len // 2\n",
    "\n",
    "        en_k, fft_k, ifft_k, ola_k = self.__init_kernel__()\n",
    "        self.register_buffer('en_k', en_k)\n",
    "        self.register_buffer('fft_k', fft_k)\n",
    "        self.register_buffer('ifft_k', ifft_k)\n",
    "        self.register_buffer('ola_k', ola_k)\n",
    "\n",
    "    def __init_kernel__(self):\n",
    "        \"\"\"\n",
    "        Generate enframe_kernel, fft_kernel, ifft_kernel and overlap-add kernel.\n",
    "        ** enframe_kernel: Using conv1d layer and identity matrix.\n",
    "        ** fft_kernel: Using linear layer for matrix multiplication. In fact,\n",
    "        enframe_kernel and fft_kernel can be combined, But for the sake of \n",
    "        readability, I took the two apart.\n",
    "        ** ifft_kernel, pinv of fft_kernel.\n",
    "        ** overlap-add kernel, just like enframe_kernel, but transposed.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: four kernels.\n",
    "        \"\"\"\n",
    "        enframed_kernel = torch.eye(self.fft_len)[:, None, :]\n",
    "        if support_clp_op:\n",
    "            tmp = fft(torch.eye(self.fft_len))\n",
    "            fft_kernel = torch.stack([tmp.real, tmp.imag], dim=2)\n",
    "        else:\n",
    "            fft_kernel = fft(torch.eye(self.fft_len), 1)\n",
    "        if self.mode == 'break':\n",
    "            enframed_kernel = torch.eye(self.win_len)[:, None, :]\n",
    "            fft_kernel = fft_kernel[:self.win_len]\n",
    "        fft_kernel = torch.cat(\n",
    "            (fft_kernel[:, :, 0], fft_kernel[:, :, 1]), dim=1)\n",
    "        ifft_kernel = torch.pinverse(fft_kernel)[:, None, :]\n",
    "        window = get_window(self.win_type, self.win_len, fftbins=False)\n",
    "\n",
    "        self.perfect_reconstruct = check_COLA(\n",
    "            window,\n",
    "            self.win_len,\n",
    "            self.win_len-self.win_hop)\n",
    "        window = torch.FloatTensor(window)\n",
    "        if self.mode == 'continue':\n",
    "            left_pad = (self.fft_len - self.win_len)//2\n",
    "            right_pad = left_pad + (self.fft_len - self.win_len) % 2\n",
    "            window = F.pad(window, (left_pad, right_pad))\n",
    "        if self.win_sqrt:\n",
    "            self.padded_window = window\n",
    "            window = torch.sqrt(window)\n",
    "        else:\n",
    "            self.padded_window = window**2\n",
    "\n",
    "        fft_kernel = fft_kernel.T * window\n",
    "        ifft_kernel = ifft_kernel * window\n",
    "        ola_kernel = torch.eye(self.fft_len)[:self.win_len, None, :]\n",
    "        if self.mode == 'continue':\n",
    "            ola_kernel = torch.eye(self.fft_len)[:, None, :self.fft_len]\n",
    "        return enframed_kernel, fft_kernel, ifft_kernel, ola_kernel\n",
    "\n",
    "    def is_perfect(self):\n",
    "        \"\"\"\n",
    "        Whether the parameters win_len, win_hop and win_sqrt\n",
    "        obey constants overlap-add(COLA)\n",
    "\n",
    "        Returns:\n",
    "            bool: Return true if parameters obey COLA.\n",
    "        \"\"\"\n",
    "        return self.perfect_reconstruct and self.pad_center\n",
    "\n",
    "    def transform(self, inputs, return_type='complex'):\n",
    "        \"\"\"Take input data (audio) to STFT domain.\n",
    "\n",
    "        Args:\n",
    "            inputs (tensor): Tensor of floats, with shape (num_batch, num_samples)\n",
    "            return_type (str, optional): return (mag, phase) when `magphase`,\n",
    "            return (real, imag) when `realimag` and complex(real, imag) when `complex`.\n",
    "            Defaults to 'complex'.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (mag, phase) when `magphase`, return (real, imag) when\n",
    "            `realimag`. Defaults to 'complex', each elements with shape \n",
    "            [num_batch, num_frequencies, num_frames]\n",
    "        \"\"\"\n",
    "        assert return_type in ['magphase', 'realimag', 'complex']\n",
    "        if inputs.dim() == 2:\n",
    "            inputs = torch.unsqueeze(inputs, 1)\n",
    "        self.num_samples = inputs.size(-1)\n",
    "        if self.pad_center:\n",
    "            inputs = F.pad(\n",
    "                inputs, (self.pad_amount, self.pad_amount), mode='reflect')\n",
    "        enframe_inputs = F.conv1d(inputs, self.en_k, stride=self.win_hop)\n",
    "        outputs = torch.transpose(enframe_inputs, 1, 2)\n",
    "        outputs = F.linear(outputs, self.fft_k)\n",
    "        outputs = torch.transpose(outputs, 1, 2)\n",
    "        dim = self.fft_len//2+1\n",
    "        real = outputs[:, :dim, :]\n",
    "        imag = outputs[:, dim:, :]\n",
    "        if return_type == 'realimag':\n",
    "            return real, imag\n",
    "        elif return_type == 'complex':\n",
    "            assert support_clp_op\n",
    "            return torch.complex(real, imag)\n",
    "        else:\n",
    "            mags = torch.sqrt(real**2+imag**2)\n",
    "            phase = torch.atan2(imag, real)\n",
    "            return mags, phase\n",
    "\n",
    "    def inverse(self, input1, input2=None, input_type='magphase'):\n",
    "        \"\"\"Call the inverse STFT (iSTFT), given tensors produced \n",
    "        by the `transform` function.\n",
    "\n",
    "        Args:\n",
    "            input1 (tensors): Magnitude/Real-part of STFT with shape \n",
    "            [num_batch, num_frequencies, num_frames]\n",
    "            input2 (tensors): Phase/Imag-part of STFT with shape\n",
    "            [num_batch, num_frequencies, num_frames]\n",
    "            input_type (str, optional): Mathematical meaning of input tensor's.\n",
    "            Defaults to 'magphase'.\n",
    "\n",
    "        Returns:\n",
    "            tensors: Reconstructed audio given magnitude and phase. Of\n",
    "                shape [num_batch, num_samples]\n",
    "        \"\"\"\n",
    "        assert input_type in ['magphase', 'realimag']\n",
    "        if input_type == 'realimag':\n",
    "            real, imag = None, None\n",
    "            if support_clp_op and torch.is_complex(input1):\n",
    "                real, imag = input1.real, input1.imag\n",
    "            else:\n",
    "                real, imag = input1, input2\n",
    "        else:\n",
    "            real = input1*torch.cos(input2)\n",
    "            imag = input1*torch.sin(input2)\n",
    "        inputs = torch.cat([real, imag], dim=1)\n",
    "        outputs = F.conv_transpose1d(inputs, self.ifft_k, stride=self.win_hop)\n",
    "        t = (self.padded_window[None, :, None]).repeat(1, 1, inputs.size(-1))\n",
    "        t = t.to(inputs.device)\n",
    "        coff = F.conv_transpose1d(t, self.ola_k, stride=self.win_hop)\n",
    "        rm_start, rm_end = self.pad_amount, self.pad_amount+self.num_samples\n",
    "        outputs = outputs[..., rm_start:rm_end]\n",
    "        coff = coff[..., rm_start:rm_end]\n",
    "        coffidx = torch.where(coff > 1e-8)\n",
    "        outputs[coffidx] = outputs[coffidx]/(coff[coffidx])\n",
    "        return outputs.squeeze(dim=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Take input data (audio) to STFT domain and then back to audio.\n",
    "\n",
    "        Args:\n",
    "            inputs (tensor): Tensor of floats, with shape [num_batch, num_samples]\n",
    "\n",
    "        Returns:\n",
    "            tensor: Reconstructed audio given magnitude and phase.\n",
    "            Of shape [num_batch, num_samples]\n",
    "        \"\"\"\n",
    "        mag, phase = self.transform(inputs)\n",
    "        rec_wav = self.inverse(mag, phase)\n",
    "        return rec_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperPreprocessor(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.stft = STFT(\n",
    "            win_len=400, win_hop=160, fft_len=400,\n",
    "\t\t\tpad_center=False # For streaming\n",
    "        )\n",
    "\t\t# self.filters = torch.from_numpy(librosa.filters.mel(sr=16000, n_fft=400, n_mels=80))\n",
    "\n",
    "\t\tself.register_buffer('filters', torch.from_numpy(librosa.filters.mel(sr=SAMPLE_RATE, n_fft=N_FFT, n_mels=N_MELS)))\n",
    "\t\n",
    "\tdef forward(self, audio_signal):\n",
    "\t\tmags, _ = self.stft.transform(audio_signal, return_type='magphase')\n",
    "\t\tmags = mags**2\n",
    "\n",
    "\t\taudio_signal = self.filters @ mags\n",
    "\t\taudio_signal = torch.clamp(audio_signal, min=1e-10).log10()\n",
    "\t\taudio_signal = (audio_signal + 4.0) / 4.0\n",
    "\n",
    "\t\treturn audio_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperEncoderALiBi(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder.to(DEVICE)\n",
    "        self.preprocessor = WrapperPreprocessor().to(DEVICE)\n",
    "\n",
    "    def forward(self, \n",
    "                audio_chunk, audio_cache, \n",
    "                conv1_cache, conv2_cache, conv3_cache,\n",
    "                k_cache, v_cache, cache_len):\n",
    "        audio_chunk = torch.cat([audio_cache, audio_chunk], dim=1)\n",
    "        audio_cache = audio_chunk[:, -(N_FFT - HOP_LENGTH):]\n",
    "\n",
    "        x_chunk = self.preprocessor(audio_chunk)\n",
    "        x_chunk = torch.cat([conv1_cache, x_chunk], dim=2)\n",
    "\n",
    "        conv1_cache = x_chunk[:, :, -1].unsqueeze(2)\n",
    "        x_chunk = F.gelu(self.encoder.conv1(x_chunk))\n",
    "\n",
    "        x_chunk = torch.cat([conv2_cache, x_chunk], dim=2)\n",
    "        conv2_cache = x_chunk[:, :, -1].unsqueeze(2)\n",
    "        x_chunk = F.gelu(self.encoder.conv2(x_chunk))\n",
    "\n",
    "        x_chunk = torch.cat([conv3_cache, x_chunk], dim=2)\n",
    "        conv3_cache = x_chunk[:, :, -1].unsqueeze(2)\n",
    "        x_chunk = F.gelu(self.encoder.conv3(x_chunk))\n",
    "\n",
    "        x_chunk = x_chunk.permute(0, 2, 1)\n",
    "\n",
    "        x_len = torch.tensor([ATTENTION_CONTEXT_SIZE[0] + ATTENTION_CONTEXT_SIZE[1] + 1]).to(DEVICE)\n",
    "        offset = torch.neg(cache_len) + ATTENTION_CONTEXT_SIZE[0]\n",
    "\n",
    "        attn_mask = self.encoder.form_attention_mask_for_streaming(ATTENTION_CONTEXT_SIZE, x_len, offset.to(DEVICE), DEVICE)\n",
    "        attn_mask = attn_mask[:, :, ATTENTION_CONTEXT_SIZE[0]:, :]\n",
    "\n",
    "        new_k_cache = []\n",
    "        new_v_cache = []\n",
    "        for i, block in enumerate(self.encoder.blocks):\n",
    "            x_chunk, layer_k_cache, layer_v_cache = block(x_chunk, mask=attn_mask, k_cache=k_cache[i], v_cache=v_cache[i])\n",
    "            new_k_cache.append(layer_k_cache)\n",
    "            new_v_cache.append(layer_v_cache)\n",
    "\n",
    "        enc_out = self.encoder.ln_post(x_chunk)\n",
    "\n",
    "        k_cache = torch.stack(new_k_cache, dim=0)\n",
    "        v_cache = torch.stack(new_v_cache, dim=0)\n",
    "        cache_len = torch.clamp(cache_len + ATTENTION_CONTEXT_SIZE[1] + 1, max=ATTENTION_CONTEXT_SIZE[0])\n",
    "\n",
    "        return enc_out, audio_cache, conv1_cache, conv2_cache, conv3_cache, k_cache, v_cache, cache_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrapperEncoderALiBi(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(2,))\n",
       "    (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,))\n",
       "    (conv3): Conv1d(768, 768, kernel_size=(3,), stride=(2,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (attn): ALiBiMultiHeadAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (alibi): AliBiRelPositionalEncoding()\n",
       "        )\n",
       "        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (preprocessor): WrapperPreprocessor(\n",
       "    (stft): STFT()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_encoder = WrapperEncoderALiBi(encoder)\n",
    "export_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_chunk = torch.zeros(1, HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH), device=DEVICE)\n",
    "audio_cache = torch.zeros(1, N_FFT - HOP_LENGTH, device=DEVICE)\n",
    "conv1_cache = torch.zeros(1, 80, 1, device=DEVICE)\n",
    "conv2_cache = torch.zeros(1, 768, 1, device=DEVICE)\n",
    "conv3_cache = torch.zeros(1, 768, 1, device=DEVICE)\n",
    "\n",
    "k_cache = torch.zeros(12, 1, ATTENTION_CONTEXT_SIZE[0], 768, device=DEVICE)\n",
    "v_cache = torch.zeros(12, 1, ATTENTION_CONTEXT_SIZE[0], 768, device=DEVICE)\n",
    "cache_len = torch.zeros(1, dtype=torch.int, device=DEVICE)\n",
    "\n",
    "r = export_encoder(\n",
    "    audio_chunk, audio_cache, \n",
    "    conv1_cache, conv2_cache, conv3_cache, \n",
    "    k_cache, v_cache, cache_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3371075/2060333728.py:31: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  x_len = torch.tensor([ATTENTION_CONTEXT_SIZE[0] + ATTENTION_CONTEXT_SIZE[1] + 1]).to(DEVICE)\n",
      "/wp/tutorials/models/encoder.py:255: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  max_audio_length = max(padding_length).detach().item()\n",
      "/wp/tutorials/models/encoder.py:255: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_audio_length = max(padding_length).detach().item()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:1959: FutureWarning: 'torch.onnx.symbolic_opset9._cast_Bool' is deprecated in version 2.0 and will be removed in the future. Please Avoid using this function and create a Cast node instead.\n",
      "  return fn(g, to_cast_func(g, input, False), to_cast_func(g, other, False))\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:663: UserWarning: Constant folding in symbolic shape inference fails: start out of range (expected to be in range of [-1, 1], but got -3) (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:441.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:1186: UserWarning: Constant folding in symbolic shape inference fails: start out of range (expected to be in range of [-1, 1], but got -3) (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:441.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    torch.onnx.export(\n",
    "        export_encoder,\n",
    "        (audio_chunk, audio_cache, conv1_cache, conv2_cache, conv3_cache, k_cache, v_cache, cache_len),\n",
    "        \"./onnx/encoder.onnx\",\n",
    "        input_names=[\"audio_chunk\", \"audio_cache\", \"conv1_cache\", \"conv2_cache\", \"conv3_cache\", \"k_cache\", \"v_cache\", \"cache_len\"],\n",
    "        output_names=[\"enc_out\", \"audio_cache\", \"conv1_cache\", \"conv2_cache\", \"conv3_cache\", \"k_cache\", \"v_cache\", \"cache_len\"],\n",
    "        export_params=True,\n",
    "\t\topset_version=17,\n",
    "\t\tdo_constant_folding=False, # Must be false for alibi\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperDecoder(nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.decoder = decoder.to(self.device)\n",
    "\n",
    "    def forward(self, token, h_n):\n",
    "        dec, h_n = self.decoder(token, h_n)\n",
    "        return dec, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrapperDecoder(\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(1025, 768)\n",
       "    (rnn): GRU(768, 768, batch_first=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = WrapperDecoder(decoder)\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = torch.tensor([[RNNT_BLANK]], dtype=torch.long, device=DEVICE)\n",
    "h_n = torch.zeros(1, 1, 768, device=DEVICE)\n",
    "\n",
    "r = decoder(token, h_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:4279: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with GRU can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    torch.onnx.export(\n",
    "        decoder,\n",
    "        (token, h_n),\n",
    "        \"./onnx/decoder.onnx\",\n",
    "        input_names=[\"token\", \"h_n\"],\n",
    "        output_names=[\"dec\", \"h_n\"],\n",
    "        export_params=True,\n",
    "        opset_version=17,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperJoint(nn.Module):\n",
    "    def __init__(self, joint):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.joint = joint.to(self.device)\n",
    "\n",
    "    def forward(self, enc, dec):\n",
    "        return self.joint(enc, dec)[0, 0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WrapperJoint(\n",
       "  (joint): Jointer(\n",
       "    (fc): Linear(in_features=768, out_features=1025, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jointer = WrapperJoint(joint)\n",
    "jointer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = torch.zeros(1, 1, 768, device=DEVICE)\n",
    "dec = torch.zeros(1, 1, 768, device=DEVICE)\n",
    "\n",
    "r = jointer(enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    torch.onnx.export(\n",
    "        jointer,\n",
    "        (enc, dec),\n",
    "        \"./onnx/jointer.onnx\",\n",
    "        input_names=[\"enc\", \"dec\"],\n",
    "        output_names=[\"output\"],\n",
    "        export_params=True,\n",
    "        opset_version=17,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '-m', 'onnxruntime.quantization.preprocess', '--input', './onnx/jointer.onnx', '--output', './onnx/jointer-infer.onnx'], returncode=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN: python -m onnxruntime.quantization.preprocess --input jointer.onnx --output jointer-infer.onnx before quantization\n",
    "import subprocess\n",
    "\n",
    "subprocess.run([\"python\", \"-m\", \"onnxruntime.quantization.preprocess\", \"--input\", \"./onnx/encoder.onnx\", \"--output\", \"./onnx/encoder-infer.onnx\"])\n",
    "subprocess.run([\"python\", \"-m\", \"onnxruntime.quantization.preprocess\", \"--input\", \"./onnx/decoder.onnx\", \"--output\", \"./onnx/decoder-infer.onnx\"])\n",
    "subprocess.run([\"python\", \"-m\", \"onnxruntime.quantization.preprocess\", \"--input\", \"./onnx/jointer.onnx\", \"--output\", \"./onnx/jointer-infer.onnx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_dynamic(\n",
    "    './onnx/encoder-infer.onnx', \n",
    "    './onnx/encoder-infer.quant.onnx',\n",
    "    weight_type=QuantType.QInt8,\n",
    "    op_types_to_quantize=['MatMul'])\n",
    "\n",
    "quantize_dynamic(\n",
    "    './onnx/decoder-infer.onnx', \n",
    "    './onnx/decoder-infer.quant.onnx',\n",
    "    weight_type=QuantType.QInt8,\n",
    "    op_types_to_quantize=['GRU'])\n",
    "\n",
    "quantize_dynamic(\n",
    "    './onnx/jointer-infer.onnx', \n",
    "    './onnx/jointer-infer.quant.onnx',\n",
    "    weight_type=QuantType.QInt8,\n",
    "    op_types_to_quantize=['MatMul'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_encoder_session = ort.InferenceSession(\"./onnx/encoder-infer.onnx\")\n",
    "ort_decoder_session = ort.InferenceSession(\"./onnx/decoder-infer.onnx\")\n",
    "ort_jointer_session = ort.InferenceSession(\"./onnx/jointer-infer.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx_online_inference(audio, ort_encoder_session, ort_decoder_session, ort_jointer_session, tokenizer):\n",
    "    if type(audio) == torch.Tensor:\n",
    "        audio = audio.cpu().numpy()\n",
    "\n",
    "    if audio.ndim == 1:\n",
    "        audio = np.expand_dims(audio, 0)\n",
    "\n",
    "    audio_cache = np.zeros((1, N_FFT - HOP_LENGTH), dtype=np.float32)\n",
    "    conv1_cache = np.zeros((1, 80, 1), dtype=np.float32)\n",
    "    conv2_cache = np.zeros((1, 768, 1), dtype=np.float32)\n",
    "    conv3_cache = np.zeros((1, 768, 1), dtype=np.float32)\n",
    "\n",
    "    k_cache = np.zeros((12, 1, ATTENTION_CONTEXT_SIZE[0], 768), dtype=np.float32)\n",
    "    v_cache = np.zeros((12, 1, ATTENTION_CONTEXT_SIZE[0], 768), dtype=np.float32)\n",
    "    cache_len = np.zeros((1,), dtype=np.int32)\n",
    "\n",
    "    h_n = np.zeros((1, 1, 768), dtype=np.float32)\n",
    "    token = np.array([[RNNT_BLANK]], dtype=np.int64)\n",
    "\n",
    "    RTF = audio.shape[1] / SAMPLE_RATE\n",
    "    seq_ids = []\n",
    "\n",
    "    start = 0\n",
    "    for i in range(0, audio.shape[1], HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH)):\n",
    "        audio_chunk = audio[:, i:i+HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH)]\n",
    "        if audio_chunk.shape[1] < HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH):\n",
    "            audio_chunk = np.pad(audio_chunk, ((0, 0), (0, HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH) - audio_chunk.shape[1])))\n",
    "\n",
    "        r = ort_encoder_session.run(\n",
    "            None,\n",
    "            {\n",
    "                \"audio_chunk\": audio_chunk,\n",
    "                \"audio_cache.1\": audio_cache,\n",
    "                \"conv1_cache.1\": conv1_cache,\n",
    "                \"conv2_cache.1\": conv2_cache,\n",
    "                \"conv3_cache.1\": conv3_cache,\n",
    "                \"k_cache.1\": k_cache,\n",
    "                \"v_cache.1\": v_cache,\n",
    "                \"cache_len.1\": cache_len\n",
    "            }\n",
    "        )\n",
    "\n",
    "        enc_out, audio_cache, conv1_cache, conv2_cache, conv3_cache, k_cache, v_cache, cache_len = r\n",
    "\n",
    "        for time_idx in range(enc_out.shape[1]):\n",
    "            curent_seq_enc_out = enc_out[:, time_idx, :].reshape(1, 1, N_STATE)\n",
    "\n",
    "            not_blank = True\n",
    "            symbols_added = 0\n",
    "\n",
    "            while not_blank and symbols_added < 3:\n",
    "                dec, new_h_n = ort_decoder_session.run(\n",
    "                    None,\n",
    "                    {\n",
    "                        \"token\": token,\n",
    "                        \"h_n.1\": h_n\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                logits = ort_jointer_session.run(\n",
    "                    None,\n",
    "                    {\n",
    "                        \"enc\": curent_seq_enc_out,\n",
    "                        \"dec\": dec\n",
    "                    }\n",
    "                )[0]\n",
    "\n",
    "                new_token = int(logits.argmax())\n",
    "\n",
    "                if new_token == RNNT_BLANK:\n",
    "                    not_blank = False\n",
    "                else:\n",
    "                    symbols_added += 1\n",
    "                    token = np.array([[new_token]], dtype=np.int64)\n",
    "                    h_n = new_h_n\n",
    "                    seq_ids.append(new_token)\n",
    "    end = time.time()\n",
    "\n",
    "    return tokenizer.decode(seq_ids), RTF / (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3371075/1346670641.py:1: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio = librosa.load(\"/wp/vn_audio/vnbacnam.m4a\", sr=SAMPLE_RATE)[0]\n",
      "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    }
   ],
   "source": [
    "audio = librosa.load(\"/wp/vn_audio/vnbacnam.m4a\", sr=SAMPLE_RATE)[0]\n",
    "audio = np.pad(audio, (16000, 0)) # add some zeros to the start of the audio for warmup\n",
    "audio = np.expand_dims(audio, 0).astype(np.float32)\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('đường xắt cao tốc bắc nam tố vận chuyển hành khách vậy nếu có chờ hàng xẻ ra sao',\n",
       " 6.103183240733414e-09)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_online_inference(audio, ort_encoder_session, ort_decoder_session, ort_jointer_session, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
